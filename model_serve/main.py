import mlflow.pyfunc
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import os

# FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸°í™”
app = FastAPI()

# MLflow Tracking URI ì„¤ì •
os.environ["MLFLOW_TRACKING_URI"] = "http://mlflow:5000"

# ëª¨ë¸ ìºì‹œ
model_cache = {}


class InferenceRequest(BaseModel):
    inputs: List[List[float]]


@app.get("/")
def read_root():
    return {"message": "MLflow FastAPI Model Serving Server is running ğŸš€"}


@app.get("/models/")
def list_models():
    """
    ë“±ë¡ëœ ëª¨ë¸ ëª©ë¡ ë°˜í™˜
    """
    import mlflow
    client = mlflow.tracking.MlflowClient()
    models = client.search_registered_models()
    return [{"name": model.name, "latest_version": model.latest_versions[0].version} for model in models]


@app.get("/models/{model_name}/versions/")
def list_model_versions(model_name: str):
    """
    íŠ¹ì • ëª¨ë¸ì˜ ë²„ì „ ëª©ë¡ ë°˜í™˜
    """
    import mlflow
    client = mlflow.tracking.MlflowClient()
    versions = client.get_latest_versions(model_name)
    return [{"version": version.version, "status": version.status} for version in versions]


@app.post("/models/predict/{model_name}/{version}/")
@app.post("/models/predict/{model_name}/")
def predict(model_name: str, version: Optional[str] = None, request: InferenceRequest = None):
    """
    íŠ¹ì • ëª¨ë¸ ë²„ì „ì„ ì‚¬ìš©í•˜ê±°ë‚˜, ë²„ì „ì´ ì—†ì„ ê²½ìš° ìµœì‹  ë²„ì „ìœ¼ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰
    """
    global model_cache
    client = mlflow.tracking.MlflowClient()

    # ë²„ì „ì´ ì œê³µë˜ì§€ ì•Šì€ ê²½ìš° ìµœì‹  ë²„ì „ ì‚¬ìš©
    if version is None:
        try:
            versions = client.get_latest_versions(model_name)
            if not versions:
                raise HTTPException(status_code=404, detail=f"No versions found for model: {model_name}")
            version = versions[0].version  # ìµœì‹  ë²„ì „ ì„ íƒ
            print(f"â„¹ï¸ Using latest version ({version}) of model '{model_name}'")
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Failed to fetch latest model version: {e}")

    # ëª¨ë¸ í‚¤ ìƒì„± (ìºì‹œ í™•ì¸ìš©)
    model_key = f"{model_name}:{version}"
    if model_key not in model_cache:
        try:
            model_uri = f"models:/{model_name}/{version}"
            print(f"â„¹ï¸ Loading model from URI: {model_uri}")
            model = mlflow.pyfunc.load_model(model_uri)
            model_cache[model_key] = model
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Failed to load model: {e}")

    # ì˜ˆì¸¡ ìˆ˜í–‰
    try:
        predictions = model_cache[model_key].predict(request.inputs)
        return {"model_name": model_name, "version": version, "predictions": predictions.tolist()}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Prediction failed: {e}")